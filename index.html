<!doctype html>
<html>

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <meta name="viewport" content="width=device-width">
  <title>Target Re-identification and Multi-Target Multi-Camera Tracking</title>

  <link rel="stylesheet" href="stylesheets/styles.css">
  <link rel="stylesheet" href="stylesheets/github-light.css">


  <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  <style>

  </style>

</head>

<body>
  <div class="wrapper">
    <header>
      <h1>ReID and MTMCT</h1>
      <!--
        <p><b>Target Re-identification and <br />Multi-Target Multi-Camera Tracking</b><br />
        A workshop in conjunction with CVPR19</p>
        -->

      <img src="img/eg_duke.png" class="figure-img img-fluid img-rounded" alt="duke">
      <img src="img/eg_market.png" class="figure-img img-fluid img-rounded" alt="duke">
      <!--<img src="img/eg_duke.png" class="figure-img img-fluid img-rounded" alt="duke">-->
      <br><br>
      <p><b>Dates</b>:<br /> 
        Paper submission deadline: 2019 March 30<br /> 
        <!--Rebuttal period: 2019 April 17-21 <br />-->
        Final decisions to authors: 2019 April 12<br /> 
        Camera ready deadline: 2019 April 17<br /><br />
        
        <!--Challenge kick-off: 2019 March 1<br /> 
        Testing data release: 2019 May 15 <br />
        Result submission deadline: 2019 June 1<br />
        Code release deadline: 2019 June 9<br />-->
        <!--Workshop: July 21, 2019-->
      </p>

      <p><b>Navigation:</b>
        <ul style="margin-top:-10px;">
          <li><a href="#program">Program</a></li>
          <li><a href="#papers">Accepted papers</a></li>
          <li><a href="#cfp">Call for papers</a></li>
          <!--<li><a href="#motivation">Problem definition and motivation</a></li>-->
          <!--<li><a href="#submission">Submission</a></li>-->
          <li><a href="#invited">Keynote speakers</a></li>
          <li><a href="#people">People involved</a></li>
          <li><a href="#contact">Contact</a></li>
        </ul>
      </p>
    </header>
    <section>

      <!--        <h2>

<br>A workshop in conjunction with CVPR 2019</h2>-->
      <h2>2nd Workshop on</h2>
      <h1 style="margin-top:-10px">
        Target Re-identification and <br/>Multi-Target Multi-Camera Tracking<br>
      </h1>
      <hr />

      <h3>In conjunction with CVPR 2019<br/> June 2019, Long Beach, California</h3>

      <p>The <a href="https://reid-mct.github.io/2017/">1st MTMCT and ReID workshop</a> was successfully held at CVPR 2017. 
        In the past two years, the MTMCT and REID community has been growing fast. As such, we are organizing this workshop 
        for a second time, aiming to gather the state-of-the-art technologies and brainstorm future directions. 
        We are especially welcoming ideas and contributions that embrace the relationship and future of MTMCT and ReID, 
        two deeply connected domains. 
        <!--A distinct feature of this workshop is a MTMCT and ReID challenge. The challenge consists of three tracks: multi-target 
        multi-camera tracking, person ReID with same train/test domains, and person ReID with distinct train/test domains.-->
        This workshop will encourage lively discussions on shaping future research directions for both academia and the industry.<br></p>
      <hr />

      <!--<h3>
        <a id="program" class="anchor" href="#designer-templates" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Program</h3>-->


      <table style="border-spacing: 80px, 100px;">
        <tr>
          <td></td>
          <td></td>
          <td></td>
        </tr>
        <tr>
          <td style="width:15%; text-align:center">Start Time</td>
          <td style="width:40%; text-align:center;">Paper/Talk Title</td>
          <td>Speaker/Author(s)</td>
        </tr>

        <tr>
          <td style="text-align:center;">8:30</td>
          <td>Welcome</td>
          <td style="text-align:center;">-</td>
        </tr>

        <tr>
          <td style="text-align:center;">8:40</td>
          <td>Invited Talk: Re-identification of Humans and Vehicles: Something Old and Something New</td>
          <td>Rama Chellappa (University of Maryland, College Park)</td>
        </tr>

        <tr>
          <td style="text-align:center;">9:10</td>
          <td>Invited Talk: Learning Adaptation from Failure for Object Identification</td>
          <td>Ying Wu (Northwestern University)</td>
        </tr>

        <tr>
          <td style="text-align:center;">9:40</td>
          <td>Invited Talk: Less is more: learning to find video highlights</td>
          <td>Kristen Grauman (Facebook AI Research)</td>
        </tr>

        <tr>
          <td style="text-align:center;">10:10</td>
          <td>Oral: Bag of Tricks and A Strong Baseline for Deep Person Re-identification</td>
          <td>Hao Luo, Youzhi Gu, Xingyu Liao, Shenqi Lai, Wei Jiang</td>
        </tr>

        <tr>
          <td style="text-align:center;">10:30</td>
          <td>Oral: State-aware Re-identification Feature for Multi-target Multi-camera Tracking</td>
          <td>Peng Li, Jiabin Zhang, Zheng Zhu, Yanwei Li, Lu Jiang, Guan Huang</td>
        </tr>
        <tr>
          <td style="text-align:center;">10:50</td>
          <td>Poster Session (Morning Break)</td>
          <td style="text-align:center;">-</td>
        </tr>
        <tr>
          <td style="text-align:center;">11:30</td>
          <td>Oral: Aggregating Deep Pyramidal Representations for Person Re-Identification</td>
          <td>Niki Martinel, Gian Luca Foresti, Christian Micheloni</td>
        </tr>


        <tr>
          <td style="text-align:center;">11:50</td>
          <td>Spotlights</td>
          <td></td>
        </tr>
        
        <tr>
          <td style="text-align:center;">-</td>
          <td>Masked Graph Attention Network for Person Re-identification</td>
          <td>Liqiang Bao, Bingpeng Ma, Hong Chang, Xilin Chen </td>
        </tr>

        <tr>
          <td style="text-align:center;">-</td>
          <td>Camera-Aware Image-to-Image Translation Using Similarity Preserving StarGAN For Person Re-identification</td>
          <td>Dahjung Chung, Edward Delp</td>
        </tr>
        
        <tr>
          <td style="text-align:center;">-</td>
          <td>In Defense of the Classification Loss for Person Re-Identification</td>
          <td>Yao Zhai, Xun Guo, Yan Lu, Houqiang Li </td>
        </tr>        
        
        <tr>
          <td style="text-align:center;">-</td>
          <td>Unsupervised Person Re-Identification with Iterative Self-Supervised Domain Adaptation</td>
          <td>Haotian Tang, Yiru Zhao, Hongtao Lu</td>
        </tr>               
        
        <tr>
          <td style="text-align:center;">-</td>
          <td>Multi-Scale Body-Part Mask Guided Attention for Person Re-identification</td>
          <td>Honglong Cai, Zhiguan Wang, Jinxing Cheng</td>
        </tr>          

        <tr>
          <td style="text-align:center;">12:30</td>
          <td>Closing Remarks</td>
          <td style="text-align:center;">-</td>
        </tr>

      </table>
      
      <!--<U>Track 1: Multi-Target Multi-Camera Tracking (MTMCT)</U><br>
      This workshop challenge aims to further push the state of the art in Multi-Target Multi-Camera Tracking. The MTMCT challenge features videos from 8 synchronized disjoint cameras recorded on the Duke University campus, with training/validation and test data. The goal is to correctly track people within and across cameras, with emphasis on correct identification. All participants are encouraged to submit their results on the <a href="https://motchallenge.net/results/DukeMTMCT/">DukeMTMCT Challenge</a> on MOTChallenge. The participating entry with the highest multi-camera IDF1 score on the test-hard sequence will be selected as the winner. Sample code from the <a href="https://github.com/ergysr/DeepCC">DeepCC tracker</a> is available. Below we list the rules of the competition.<br>

      <br>
      <a href="https://mctreid.com/index/">Registration</a> is open.
      <br><br>
      
      <button onclick="myFunction1()">Rules</button>

        <div id="rules1" style="display: none">
          <ul class="list-group">
            <li class="list-group-item">Any person detector, pre-trained neural network, or public data related to DukeMTMC can be used.</li>
            <li class="list-group-item">Manual labeling of additional training data is not allowed. Existing data should be sufficient.</li>
            <li class="list-group-item">Each participating research group can submit to the server a maximum of 4 times, even if the research group consists of multiple researchers. [<a href="http://vision.cs.duke.edu/DukeMTMC/details.html#evaluation">Guidelines</a>]</li>
            <li class="list-group-item">To facilitate further advances in the field participating groups are required to release code publicly one week before the workshop, to be considered eligible, and all repositories will be shared with the community. The organizing committee reserves the right to revoke a prize if the winning method's code does not reproduce scores similar to the submission.</li>
            <li class="list-group-item">A paper submission to the workshop is encouraged but not required.</li>
            <li class="list-group-item">If you think a rule is unclear or not covered in this list, please contact the organizers.</li>
            
          </ul>
        </div>
      <br>
      <br>
          
      
      <U>Track 2: Person Re-identification with the Same Train / Test Domain</U><br>
      This track will evaluate the scalability of ReID models. The setup consists of person images from the DukeMTMC dataset, categorized into a training/validation set and a test set. For each query image, participating entries are expected to rank the gallery set in order of decreasing similarity, with the intent that highly ranked images are co-identical to the query. The method with the highest mAP score will be selected as the winner, and rank-1 accuracy will be used as a tie-breaker if necessary.<br>
      <br>
      <a href="https://mctreid.com/index/">Registration</a> is open.
      <br><br>

      <button onclick="myFunction2()">Rules</button>

            <div id="rules2" style="display: none">
              <ul class="list-group">
                <li class="list-group-item">Only person images from the DukeMTMC dataset can be used, while images from other datasets are not allowed.</li> 
                <li class="list-group-item">For training/validation any images from the DukeMTMC trainval set can be used. Participants can either extract images from the DukeMTMC videos (more samples but time consuming), or use existing datasets that have already extracted samples for each identity of trainval. [<a href="https://github.com/layumi/DukeMTMC-reID_evaluation">DukeMTMC-reID</a>][<a href="https://github.com/NEU-Gou/DukeReID">DukeMTMC4ReID</a>]</li>
                <li class="list-group-item">Methods that rely on deep learning are allowed to initialize their weights with standard ImageNet pre-trained models.</li>
                <li class="list-group-item">Using inputs such as pose estimation, attributes, foreground masks, semantic segmentation, is allowed as long as these inputs are obtained using off-the-shelf, publicly available methods. It is not allowed however to re-train these methods using Duke data. Tuning the free parameters of these methods is allowed.</li>
                <li class="list-group-item">All images from the DukeMTMC trainval set can be used when preparing the final models. Images from the test set are not.</li>
                <li class="list-group-item">Use of training images from other person ReID datasets is not allowed for this challenge. It is well-known that more training data helps, so the focus is on researching innovative methods.</li>
                <li class="list-group-item">Re-ranking methods are allowed.</li>
                <li class="list-group-item">Test data for this challenge will be released in the form of one zip file with 20k+ images. The identity labels for these images are kept secret. The test set includes images that will be used for evaluation (query and gallery images) and a large number of distractors. The test set will be available on May 15. </li>
                <li class="list-group-item">The test data will contain a single folder of images indexed from 1 to N. For each image in the test data, participants should provide a list of the 2k indices of the most similar images. The submission file will then be evaluated on the test server. [Link TBA]</li>
                <li class="list-group-item">Each participating research group can submit to the server a maximum of 4 times, even if the research group consists of multiple researchers.</li>
                <li class="list-group-item">To facilitate further advances in the field participating groups are required to release code publicly one week before the workshop, to be considered eligible, and all repositories will be shared with the community. The organizing committee reserves the right to revoke a prize if the winning method's code does not reproduce scores similar to the submission.</li>
                <li class="list-group-item">If you think a rule is unclear or not covered in this list, please contact the organizers.</li>
                
              </ul>
            </div>
            <br>
            <br>

      
      <U>Track 3: Person Re-identification with Different Train / Test Domains</U><br>
      To evaluate the generalization ability of algorithms, this track will ask teams to use the publicly available <a href="http://www.liangzheng.org/Project/project_reid.html">Market-1501 dataset</a>
      for training / validation. In this challenge the setup consists of person images from the Market-1501 dataset used for training and validation, and a test set from DukeMTMC with secret labels (to be released). For each query image in the test set, participating entries are expected to rank the gallery set in order of decreasing similarity, with the intent that highly ranked images are co-identical to the query. The method with the highest mAP score will be selected as the winner, and rank-1 accuracy will be used as a tie-breaker if necessary.<br>
      <br>
      <a href="https://mctreid.com/index/">Registration</a> is open.
      <br><br>
      <button onclick="myFunction3()">Rules</button>

            <div id="rules3" style="display: none">
              <ul class="list-group">
                <li class="list-group-item">Only person images from the Market-1501 dataset can be used for training. DukeMTMC images are <i>not</i> to be used for training or validation, neither for unsupervised domain adaptation, and we expect that all participants will act honorably.</li> 
                <li class="list-group-item">Validation on other datasets is allowed.</li>
                <li class="list-group-item">Methods that rely on deep learning are allowed to initialize their weights with standard ImageNet pre-trained models.</li>
                <li class="list-group-item">Using inputs such as pose estimation, attributes, foreground masks, semantic segmentation, is allowed as long as these inputs are obtained using off-the-shelf, publicly available methods. It is not allowed however to re-train these methods using DukeMTMC or Market-1501 data. Tuning the free parameters of these methods is allowed.</li>
                <li class="list-group-item">All 1501 identities from Market are allowed to be used when preparing the final models. Images from the test set or from external datasets are not.</li>
                <li class="list-group-item">Use of training images from other person ReID datasets is not allowed for this challenge. It is well-known that more training data helps, so the focus is on researching innovative methods.</li>
                <li class="list-group-item">Re-ranking methods are allowed.</li>
                <li class="list-group-item">Test data for this challenge will be released in the form of one zip file with 20k+ images. The identity labels for these images are kept secret. The test set includes images that will be used for evaluation (query and gallery images) and a large number of distractors. The test set will be available on May 15. </li>
                <li class="list-group-item">The test data will contain a single folder of images indexed from 1 to N. For each image in the test data, participants should provide a list of the 2k indices of the most similar images. The submission file will then be evaluated on the test server. [Link TBA]</li>
                <li class="list-group-item">Each participating research group can submit to the server a maximum of 4 times, even if the research group consists of multiple researchers.</li>
                <li class="list-group-item">To facilitate further advances in the field participating groups are required to release code publicly one week before the workshop, to be considered eligible, and all repositories will be shared with the community. The organizing committee reserves the right to revoke a prize if the winning method's code does not reproduce scores similar to the submission.</li>
                <li class="list-group-item">If you think a rule is unclear or not covered in this list, please contact the organizers.</li>
                
              </ul>
            </div>
        <br>
        <br>


      <br>-->
      <h3>
        <a id="papers" class="anchor" href="#designer-templates" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Accepted
        papers</h3>
        1. Bag of Tricks and A Strong Baseline for Deep Person Re-identification. </br>Hao Luo (Zhejiang University)*; Youzhi Gu (Zhejiang University); Xingyu Liao (University of Science and Technology of China); 
  Shenqi Lai (Xi'an Jiaotong University); Wei Jiang (Department of Control Science and Engineering, Zhejiang University) </br></br>

        2. Masked Graph Attention Network for Person Re-identification. </br>Liqiang Bao (University of Chinese Academy of Sciences); Bingpeng MA (University of Chinese Academy of Sciences)*; Hong Chang (Chinese Academy of Sciences); 
Xilin Chen (Institute of Computing Technology, Chinese Academy of Sciences)</br></br>

        3. State-aware Re-identification Feature for Multi-target Multi-camera Tracking. </br>
        Peng Li (Beijing University of Posts and Telecommunications)*; Jiabin Zhang (Institute of Automation, Chinese Academy of Sciences); Zheng Zhu (Institute of Automation, Chinese Academy of Sciences); Yanwei Li (Institute of Automation, CAS; 
University of Chinese Academy of Sciences); Lu Jiang (Horizon Robotics); Guan Huang (Horizon Robotics)</br></br>

        4. Camera-Aware Image-to-Image Translation Using Similarity Preserving StarGAN For Person Re-identification.</br>
        Dahjung Chung (Purdue University)*; Edward Delp (Purdue University)</br></br>

        5. In Defense of the Classification Loss for Person Re-Identification.</br>
        Yao Zhai (University of Science and Technology of China)*; Xun Guo (Microsoft Research Asia); Yan Lu (Microsoft Research Asia); Houqiang Li (University of Science and Technology of China)</br></br>
        
        6. Unsupervised Person Re-Identification with Iterative Self-Supervised Domain Adaptation.</br>
        Haotian Tang (Shanghai Jiaotong University); Yiru Zhao (Shanghai Jiao Tong University); Hongtao Lu (Shanghai Jiao Tong University)*.</br></br>

        7. Aggregating Deep Pyramidal Representations for Person Re-Identification. </br>
        Niki Martinel (University of Udine)*; Gian Luca Foresti (University of Udine, Italy); Christian Micheloni (University of Udine, Italy)</br></br>

        8. Multi-Scale Body-Part Mask Guided Attention for Person Re-identification. </br>
        Honglong Cai (Suning USA R&D Center)*; Zhiguan Wang (Suning USA R&D center); Jinxing Cheng (Suning USA R&D Center)</br></br>

      <h3>
        <a id="cfp" class="anchor" href="#designer-templates" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Call
        for papers</h3>
      In target re-identification, we define a query as a bounding box of a targe-of-interest such as a pedestrian or a vehicle. We define a database 
      as a collection of image bounding boxes of arbitrary pedestrians or vehicles. Target re-identification aims to find all the database images
      of the same target as the query. In multi-target multi-camera tracking, we use videos captured by multiple cameras. This task aims to place tight 
      bounding boxes to all the targets (e.g. pedestrians). The bounding boxes are partitioned into trajectories, a set of boxes that bound a
      unique target, ordered by time.
      
      In this full-day workshop, we will have invited speakers, poster sessions, oral presentations, as well as a summary of the challenge. 
      We encourage authors to explore the connections between the fields of ReID and MTMCT and some novel ideas. Examples of such questions are:<br>
      <ul>
        <li>How to define and improve the scalabilityof a MTMCT or ReID system?</li>
        <li>How to deal with large-scale indexing and optimization in ReID and MTMCT?</li>
        <li>How much do initial detections influence performance in MTMCT or ReID?</li>
        <li>How to improve the generalization ability of a MTMCT or ReID system?</li>
        <li>How and which ReID descriptors can be integrated into MTMCT systems?</li>
        <li>What can we learn by evaluating a MTMCT system in terms of ReID (and vice-versa)?</li>
        <li>How can ReID and MTMCT benefit each other?</li>
        <li>How can MTMCT and ReID capitalize on recent large-scale datasets?</li>
        <li>Do semantic attributes help in matching identities in ReID and MTMCT?</li>
      </ul>


      <h3>
        <a id="submission" class="anchor" href="#designer-templates" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Submission</h3>
      <p>
        To submit a new paper to the workshop, you have to do so through the <a href="https://cmt3.research.microsoft.com/TRMTMCT2019/Submission/Index">CMT website</a>.
        The workshop paper submissions should be in the same format as the main conference. Please refer to the <a href="http://cvpr2019.thecvf.com/submission/main_conference/author_guidelines">CVPR 2019 author guidelines</a> for more details.
      </p>

      <h3>
        <a id="invited" class="anchor" href="#designer-templates" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Invited
        speakers
      </h3>
      
      <img src="img/rama.jpg" width="40px" style="float: left; margin-right: 10px;" />
      <p><b>Rama Chellappa</b> </br>
        University of Maryland</p></br>

      <img src="img/grauman.jpg" width="40px" style="float: left; margin-right: 10px;" />
      <p><b>Kristen Grauman</b> </br>
        Facebook</p></br>

      <img src="img/ying.jpg" width="40px" style="float: left; margin-right: 10px;" />
      <p><b>Ying Wu</b> </br>
       Northwestern University</p></br>
      <br>
      <h3>
        <a id="people" class="anchor" href="#designer-templates" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>People
        involved
      </h3>
      <b>Organizers:</b><br>
      <img src="img/ristani.jpg" class="figure-img img-fluid img-rounded" alt="" height="60px;">
      <img src="img/liang.jpg" class="figure-img img-fluid img-rounded" alt="" height="60px;">
      <img src="img/eddy.png" class="figure-img img-fluid img-rounded" alt="" height="60px;">
      <img src="img/jwang.jpg" class="figure-img img-fluid img-rounded" alt="" height="60px;">
      <img src="img/zhang.jpg" class="figure-img img-fluid img-rounded" alt="" height="60px;">
      <img src="img/gong.jpg" class="figure-img img-fluid img-rounded" alt="" height="60px;">
      <img src="img/tian.jpg" class="figure-img img-fluid img-rounded" alt="" height="60px;">
      <img src="img/tomasi.jpg" class="figure-img img-fluid img-rounded" alt="" height="60px;">
      <img src="img/richard.jpg" class="figure-img img-fluid img-rounded" alt="" height="60px;">
      <br> Ergys Ristani (Duke University)<br>  Liang Zheng (Australian National University)<br> Xiatian Zhu (Vision Semantics Limited)<br> 
      Jingdong Wang (Microsoft Research)<br> Shiliang Zhang (Peking University)<br>  Shaogang Gong (Queen Mary University of London)<br> 
      Qi Tian (Noah Ark’s Lab, Huawei)<br> Carlo Tomasi (Duke University)<br> Richard Hartley (Australian National University)<br>    <br>

      <!--<b>Program committee chairs:</b><br>
      TBD.<br><br>-->
      <!--<img src="img/calderara.jpg" class="figure-img img-fluid img-rounded" alt="" height="60px;">
      <img src="img/snoek.jpg" class="figure-img img-fluid img-rounded" alt="" height="60px;">
      <img src="img/jwang.jpg" class="figure-img img-fluid img-rounded" alt="" height="60px;">
      <img src="img/zhang.jpg" class="figure-img img-fluid img-rounded" alt="" height="60px;">
      <br/> Simone Calderara (University of Modena and Reggio Emilia)<br> Cees G.M. Snoek (University of Amsterdam)<br> Jingdong
      Wang (Microsoft Research)<br> Shiliang Zhang (Peking University)<br><br>-->

      <b>Program committee members:</b> Dapeng Chen, Wei-shi Zheng, Francesco Solera, Weiyao Lin, Giuseppe Lisanti, Slawomir Bak,
      Eyasu Zemene Mequanit, Li Zhang, Elyor Kodirov, Ying Zhang, Mang Ye, Zhedong Zheng, Zhun Zhong, Yonatan Tariku Tesfaye, Wenhan Luo, 
      Srikrishna Karanam, Hanxiao Wang.<br><br>

      <!--<table>
  <tr><td>Horst Bischof</td><td>Roman Pflugfelder</td></tr>
  <tr><td>Octavia Camps</td><td>Amit Roy-Chowdhury</td></tr>
  <tr><td>Andrea Cavallaro</td><td>Peter Roth</td></tr>
  <tr><td>Chen Change Loy</td><td>Ling Shao</td></tr>
  <tr><td>Gerard Medioni</td><td>Xinchao Wang</td></tr>
  <tr><td>Christian Micheloni</td><td>Liang Zheng</td></tr>
  <tr><td>Vittorio Murino</td><td></td></tr>
</table>
-->

      <h3>
        <a id="contact" class="anchor" href="#designer-templates" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Contact</h3>
      For any information, please send an <a href="mailto:liang.zheng@anu.edu.au;eddy@visionsemantics.com;ristani@cs.duke.edu;jingdw@microsoft.com">e-mail</a>      to Ergys Ristani, Xiatian Zhu, Liang Zheng and Jingdong Wang.

    </section>
    <footer>

    </footer>
  </div>
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.0/css/bootstrap.min.css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.0/js/bootstrap.min.js"></script>

<script>
function myFunction1() {
  var x = document.getElementById("rules1");
  if (x.style.display === "none") {
    x.style.display = "block";
  } else {
    x.style.display = "none";
  }
}

function myFunction2() {
  var x = document.getElementById("rules2");
  if (x.style.display === "none") {
    x.style.display = "block";
  } else {
    x.style.display = "none";
  }
}

function myFunction3() {
  var x = document.getElementById("rules3");
  if (x.style.display === "none") {
    x.style.display = "block";
  } else {
    x.style.display = "none";
  }
}
</script>

  <!--<script src="./javascripts/scale.fix.js"></script>-->
  <script>
    (function (i, s, o, g, r, a, m) {
      i['GoogleAnalyticsObject'] = r;
      i[r] = i[r] || function () {
        (i[r].q = i[r].q || []).push(arguments)
      }, i[r].l = 1 * new Date();
      a = s.createElement(o),
        m = s.getElementsByTagName(o)[0];
      a.async = 1;
      a.src = g;
      m.parentNode.insertBefore(a, m)
    })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');

    ga('create', 'UA-89846544-1', 'auto');
    ga('send', 'pageview');


    $("#toggle-link").click(function () {
      $("#dis-similarities").toggle("slow");
    });
  </script>

</body>

</html>
